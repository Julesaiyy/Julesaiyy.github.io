<!doctype html>
<html>

<head>

<!-- 3651057137 -->

<title>Julesaiyy’s Homepage</title>

<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Julesaiyy"> 
<meta name="description" content="Julesaiyy's home page">
<link rel="stylesheet" href="css/jemdoc.css" type="text/css" />

<script>
   function showPubs(id) {
  if (id == 0) {
    document.getElementById('pubs').innerHTML = document.getElementById('pubs_selected').innerHTML;
    document.getElementById('select0').style = 'text-decoration:underline;color:#000000';
    document.getElementById('select1').style = '';
  } else {
    document.getElementById('pubs').innerHTML = document.getElementById('pubs_by_topic').innerHTML;
    document.getElementById('select1').style = 'text-decoration:underline;color:#000000';
    document.getElementById('select0').style = '';
  }
}

</script>

</head>

<body>

<div id="layout-content" style="margin-top:25px">


<table>
	<tbody>
		<tr>
			<td width="75%">
				<div id="toptitle">
					<h1>Julesaiyy <h1>
				</div>
		<p>
      <!-- Southwest Jiaotong University (SWJTU) </br> -->
      </br>
      Email: Julesaiyy@gmail.com </br>
		</p>
		<p>
			<a href="https://github.com/Julesaiyy"><img src="assets/logos/github_logo.png" height="30px"></a>&nbsp;&nbsp;
			<a href="https://scholar.google.com"><img src="assets/logos/google_logo.png" height="30px"></a>&nbsp;&nbsp;
			<!-- <a href="https://cn.linkedin.com/in/dong-an-9b8895159?trk=people-guest_people_search-card"><img src="assets/logos/linkedin_logo.png" height="30px"></a>&nbsp;&nbsp; -->
      <!-- <a href="https://twitter.com/andongverse"><img src="assets/logos/twitter_logo.png" height="30px"></a>&nbsp;&nbsp; -->
		</p>
			</td>

			</td>
			<td width="25%">
				<img src="assets/imgs/ad.png" width="80%"/>
			</td>
		<tr>
	</tbody>
</table>

<h2>News</h2>
<ul>
  <!-- <li>07 / 2023: &nbsp; One paper is accepted by <b>ICCV 2023!</b></li>
  <li>10 / 2022: &nbsp; One paper is accepted by <b>BMVC 2022!</b></li>
  <li>08 / 2022: &nbsp; We won <b>2nd place</b> in CSIG 2022 <a href="https://yuankaiqi.github.io/REVERIE_Challenge/">REVERIE Challenge</a>!</li>
  <li>07 / 2022: &nbsp; We won <b>2nd place</b> in IJCAI-ECAI 2022 <a href="http://ucsc-real.soe.ucsc.edu:1995/Competition.html">Noisy Labels Challenge</a>! Congratulations to WeiChen!</li>
  <li>06 / 2022: &nbsp; We won <b>1st place</b> in CVPR 2022 <a href="https://embodied-ai.org/">Embodied AI Workshop</a>: <a href="https://ai.google.com/research/rxr/habitat">RxR-Habitat Competition</a>!</li>
  <li>09 / 2021: &nbsp; One paper is accepted by <b>NeurIPS 2021!</b></li>
  <li>07 / 2021: &nbsp; One paper is accepted by <b>ACM MM2021!</b></li> -->
</ul>

<h2>Biography</h2> 
<p>
  Here is Julesaiyy. His research interests include computer vision (CV), data mining (DM), and natural language processing (NLP). Specifically, few-shot learning, recommendation systems, and medical image processing are included.
  </p>
<!-- <p>
  His research interests include computer vision and pattern recognition, with a focus on Embodied-AI and Multimodal Leaning.
</p> -->

<!-- <h2> Selected Journal Papers</h2> 
<ul>
</ul> -->
  
  
<!-- <h2>Selected Conference Papers</h2> 
<ul>
</ul> -->

<h2>Projects</h2
<ul>
</ul>
<h2>Publications</h2> 
<ul>

  <li>
    <p><strong>MCTE: Marrying Convolution and Transformer Efficiently for End-to-End Medical Image Segmentation</strong></p>
    <p><strong>Jiuqiang Li</strong></p>
    <p>International Conference on Image Processing (<strong>ICIP</strong>), 2023. 
      <a href="https://doi.org/10.1109/ICIP49359.2023.10222041" target="_self">[Paper]</a>
      <a href="https://github.com/Julesaiyy/MCTE" target="_self">[Code]</a>
      <a href="assets/bib/li2023mcte.bib" target="_self">[BibTex]</a></p>
  </li>

  <li>
    <p><strong>CAME: Convolution and Attention Construct Multi-Scale Neural Network Efficiently for Medical Image Classification</strong></p>
    <p><strong>Jiuqiang Li</strong></p>
    <p>International Conference on Systems, Man, and Cybernetics (<strong>SMC</strong>), 2023 (coming soon). 
      <a href="https://doi.org/" target="_self">[Paper]</a>
      <a href="https://github.com/Julesaiyy/CAME" target="_self">[Code]</a>
      <a href="assets/bib/li2023came.bib" target="_self">[BibTex]</a></p>
  </li>

  <li>
    <p><strong>EACCNet: Enhanced Auto-Cross Correlation Network for Few-Shot Classification</strong></p>
    <p><strong>Jiuqiang Li</strong></p>
    <p>International Conference on Knowledge Science, Engineering and Management (<strong>KSEM</strong>), 2023. 
      <a href="https://doi.org/10.1007/978-3-031-40283-8_30" target="_self">[Paper]</a>
      <a href="https://github.com/Julesaiyy/EACCNet" target="_self">[Code]</a>
      <a href="assets/bib/li2023eaccnet.bib" target="_self">[BibTex]</a></p>
  </li>

  <li>
    <p><strong>LGEFE: Effective Local-Global-External Feature Extraction for 3D Point Cloud Classification</strong></p>
    <p><strong>Jiuqiang Li</strong></p>
    <p>International Joint Conference on Neural Networks (<strong>IJCNN</strong>), 2023. 
      <a href="https://doi.org/10.1109/IJCNN54540.2023.10191638" target="_self">[Paper]</a>
      <a href="https://github.com/Julesaiyy/LGEFE" target="_self">[Code]</a>
      <a href="assets/bib/li2023lgefe.bib" target="_self">[BibTex]</a></p>
  </li>

</ul>


<!-- <h2>Competitions</h2> 
<ul>

  <table class="imgtable"><tr><td>
    <img src="assets/logos/rxrhabitat_challenge.jpg" alt="alt text" width="80" height="80" /> &nbsp;</td>
    <td align="left">
    <p> 
      <b>RxR-Habitat Vision-and-Language Navigation Challenge @ CVPR 2022 </b>.  
      Our team Joyboy (<b>Dong An</b>*, Zun Wang*, Yangguang Li, Yi Wang, Yicong Hong, Yan Huang, Liang Wang, Jing Shao) is the <font color="#FF0000">winner</font>. See details here: <a href="https://ai.google.com/research/rxr/habitat">Results of RxR-Habitat 2022</a>.
    </p>
  </td></tr></table>

  <table class="imgtable"><tr><td>
    <img src="assets/logos/reverie_challenge.jpg" alt="alt text" width="80" height="80" /> &nbsp;</td>
    <td align="left">
    <p> 
      <b>REVERIE Challenge @ CSIG 2022</b>. 
      Our team TouchFish (<b>Dong An</b>, Yifei Su, Shuanglin Sima, Hongyuan Yu, Weichen Yu, Yan Huang) is the <font color="#FF0000">runner-up</font> of both channels. 
      See details here: <a href="https://yuankaiqi.github.io/REVERIE_Challenge/challenge_2022.html">Results of REVERIE Challenge 2022</a>.
    </p>
    </td></tr></table>

  <table class="imgtable"><tr><td>
    <img src="assets/logos/ijcai_challenge.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
    <td align="left">
    <p> 
      <b>Learning and Mining with Noisy Labels Challenge @ IJCAI-ECAI 2022</b>.  
      Our team (Weichen Yu, Hongyuan Yu, Yan Huang, <b>Dong An</b>, Keji He, Zhipeng Zhang, Xiuchuan Li, Liang Wang) is the <font color="#FF0000">runner-up</font> of task 1-1 and <font color="#FF0000">2nd runner-up</font> of task 1-2. 
      See details here: <a href="http://ucsc-real.soe.ucsc.edu:1995/Competition.html">Results</a>.
    </p>
    </td></tr></table>

</ul> 
  
  
<h2> Professional Activities</h2> 
<ul>
  <li><p>Winner invited talk at Embodied-AI Workshop @ CVPR 2022 </p></li>
  <li><p>Reviewer: ACM MM 2021, IJCAI 2023 </p></li>
</ul>
  
  
<h2> Honors and Awards</h2> 
<ul> 
  <li><p>2022, <font color="#FF0000">Winner</font> of RxR-Habitat Challenge, CVPR 2022 [<a href="assets\cert\rxr-habitat-cert.pdf" target="blank">RxR-Habitat Certificate</a>]</p> </li>
  <li><p>2022, <font color="#FF0000">Runner-up</font> of REVERIE Challenge, CSIG 2022 [<a href="assets\cert\赛道1 touchfish.pdf" target="blank">REVERIE Certificate_1</a>] [<a href="assets\cert\赛道2 touchfish.pdf" target="blank">REVERIE Certificate_2</a>]</p> </li>
  <li><p>2022, <font color="#FF0000">Runner-up</font> of Learning and Mining with Noisy Labels Challenge, IJCAI-ECAI 2022 [<a href="assets\cert\noisy-label-cert.pdf" target="blank">Noisy-Labels Certificate</a>]</p> </li>
  <li><p>2021, Outstanding Student of CAS</p> </li>
</ul>


<table width="100%"> 
	<tr> 
		<td align="center">&copy; Dong An | Last update: May 2023</td>
	</tr> 
</table> -->

</div>


</body>

</html>

<!-- reference: https://marsaki.github.io/ -->
